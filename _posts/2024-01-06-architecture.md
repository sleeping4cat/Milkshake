---
tags: Comparative-study
---

### Architecture not Data: Future of Artificial Intelligence Research

Modern Artificial Intelligence is dominated by two schools of thoughts. Firstly, Architecture is the actual innovation factor. Secondly, Datasets are the advance factor. Both have their valid merits and arguments, and their respective groups consist of prominent researchers and individuals in the field. 

In this creative piece, I highlight my personal opinion why Architecture is the clear school of thought that will fuel future innovation and advances and datasets will be encapsulated in this school of thought as a prerequisite rather than prevailing as a standalone school of thought and innovation factor. 

Datasets schools of thought: I mean quite a few arguments and notions cascaded into a singular word. In this school of thought, it is perceived, innovation and advances can be achieved if Deep Learning Models are scaled (parameters-wise) and fed unique and high-quality annotated data (supervised learning). 

It was a thought, initially developed in the 2011s, through the inception of AlexNet, an innovative Computer Vision model that was a technical marvel both from the perspective of Architecture and Datasets school of thought. Researchers designed a simple yet elegant architecture and fed millions of data-points and trained it. Having millions of parameters allowed it to learn complex relationships and patterns from millions of training points. It was the first moment, when an Artificial Intelligence model could provide valuable real-world application. 

Later, the Inception of Transformers model fueled the school of thought further and Open Source Artificial intelligence development accelerated the notion: Having a large dataset and millions of parameters is enough to drive innovation. Causing researchers and students alike prioritising datasets school of thought and slowing development in Architecture advances. 

Highlighting Architecture advances still happens and happening but just enough to meet necessity and publish papers rather than contribute unique and advanced innovations that happened in past decades, that resulted in Liquid Algorithms and reservoir computing. 

Architecture school of thought signifies a notion: where researchers pay attention to develop architecture that works closely to the human brain and learns a task with a minimal amount of data-points. Further, high-quality dataset is prerequisite within set rules and boundaries than the dumping high-quality annotated data without any definite reasoning except it aligns with the desired and downstream tasks (seen in latter field)

Eliza, Llama and Mistral are leading examples how architecture advances can drastically change and advance the field of Artificial Intelligence. Eliza, a chatbot created in the past Century, capable of holding high-level human-level conversation with a limited amount of data-points through sheer technicalities of architecture. 

Llama, a recent model unveiled by Meta AI (FAIR), capable of competing against OpenAI’s GPT family models and outcompeting in a few tasks. 

Mistral, a variant of Llama and capable of outcompeting Llama and competing against state-of-art OpenAI models. 

Where’s the field headed and future? If we analyse in unbiased fashion, OpenAI’s GPT models are exceptional examples of Datasets school of thought because it offers both high parameters count within the model and taps into unique and proprietary and high-quality datasets. Rumours suggest it may be a 1.7 Trillion parameters model. 

Compared to that, Llama and Mistral are quite small models in parameters counts and rely largely on Open Source datasets. In Llama’s case, the biggest model consists of 70 Billion parameters and it is a standalone one-model. Meta, cleverly used rotary positional embedding research in developing a GPT-killer. 

Analysis of benchmarks and talent acquisition by OpenAI labs of Llama researchers, indicates that the future for solely Datasets school of thought looks bleak. Because proponents of the school like OpenAI are diverting to the Architecture school of thought, and labs which still persist in relying on datasets are unable to find their competitive advantage unless it is being a niche field with a niche dataset. Yet, another shifting of paradigm is happening. 

Then, what was the point of writing this? A valid one. In recent times, researchers and respected institutions leaned hugely on datasets of school of thought, a concept visible if recent research papers on arXiv, Conference Journals and institutions are read. A huge number of startups and respected researchers still support this thought and believe it to be the path to achieve Artificial General Intelligence for humanity. 

Paradigm shift is happening but the school of thought: Datasets is not faded and a good portion believes in its success in the quest to achieve AGI (artificial general intelligence). Looking at these, a comparative analytical creative piece is valuable to visualise the current state of the Artificial Intelligence field, its paradigm (+shift) and the future of research in this field. 

What’s the future? A likely future, Architecture design and research being forefront in the advances and innovation of Artificial Intelligence field and has been utilised to achieve Artificial General Intelligence. Better intelligence systems of those already discovered have been invented and to achieve downstream tasks with higher accuracy in significantly lower model size and data-points. Having high-quality dataset has become a prerequisite in research and training rather than being a sole contender in advances of the Artificial Intelligence field. 

Slightly different scenario: Datasets school of thought exists but in a highly limited way.

Concluding, Architecture will become the future of Artificial Intelligence. Though a different future might happen but that’s not evident from the current state of the field. 
